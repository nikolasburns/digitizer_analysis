{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from ipywidgets import IntProgress\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib\n",
    "import matplotlib.collections as mcoll\n",
    "import random\n",
    "\n",
    "# Local imports\n",
    "from searchdir import searchdir\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Constants for the Notebook\n",
    "path_to_dir = \"/home/nikolas/Documents/Research/PMT_Testing/Map\"\n",
    "paths = searchdir(path_to_dir, file_extension=\".h5\")\n",
    "\n",
    "# Scale Factors\n",
    "mppc_DIGITIZER_FULL_SCALE_RANGE = 2.5 # Vpp\n",
    "mppc_DIGITIZER_RESOLUTION = 12 # bits\n",
    "digiCounts = 2.**mppc_DIGITIZER_RESOLUTION\n",
    "verticalScaleFactor = 1.e3*mppc_DIGITIZER_FULL_SCALE_RANGE/digiCounts; # mV/bank\n",
    "mppc_DIGITIZER_SAMPLE_RATE = 3200000000 # S/s\n",
    "horizontalScaleFactor = 1.e9/mppc_DIGITIZER_SAMPLE_RATE #ns/sample\n",
    "\n",
    "print(paths)\n",
    "\n",
    "Run_IDs = [s.split(\"/\")[-1].split(\".\")[0][-3:] for s in paths]\n",
    "RunPathMap = {Run: path for Run, path in zip(Run_IDs, paths)}\n",
    "PathRunMap = {path: Run for Run, path in zip(Run_IDs, paths)}\n",
    "Run_IDs.sort()\n",
    "RunLocMap = {Run: i for i, Run in enumerate(Run_IDs)}\n",
    "\n",
    "print(RunLocMap)\n",
    "\n",
    "# print(Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSignal(path:str, channels:list[str], highpass=600, lowpass=2000, filterChannel=None, timeThreshold=80, ADCThreshold=800, peakMinTimeThreshold=20):\n",
    "    \"\"\"Filters waveforms based on highpass and lowpass filter. Can be used to drop oversaturated datat (lowpass) and to drop outliers (highpass).\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the HDF file containing the waveforms.\n",
    "        channels (list[str]): A list of channels in the HDF keys ([\"Ch0\", \"Ch1\", \"Ch2\", \"Ch3\"]).\n",
    "        highpass (int, optional): Highpass filter. Defaults to 600.\n",
    "        lowpass (int, optional): Lowpass filter. Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    print(f\"Cleaning waveforms in: {path}\")\n",
    "    DATA = {ch: None for ch in channels}\n",
    "    for channel in channels:\n",
    "        df = pd.read_hdf(path, key=channel)\n",
    "        \n",
    "        # Check if filters are different for each channel\n",
    "        hp = highpass\n",
    "        lp = lowpass\n",
    "        if type(highpass) is dict:\n",
    "            if filterChannel is None:\n",
    "                filterChannel = channel\n",
    "            hp = highpass[filterChannel]\n",
    "        if type(lowpass) is dict:\n",
    "            if filterChannel is None:\n",
    "                filterChannel = channel\n",
    "            lp = lowpass[filterChannel]\n",
    "            \n",
    "        if highpass is None and lowpass is None:\n",
    "            DATA[channel] = df\n",
    "            continue\n",
    "        elif highpass is None:\n",
    "            mask = (df.min(axis=1) <= lp)\n",
    "        elif lowpass is None:\n",
    "            mask = (df.max(axis=1) >= hp)\n",
    "        else:\n",
    "            print(f\"Applying filters: [{hp}:{lp}] on {filterChannel}\")\n",
    "            mask = ((df <= lp) & (df >= hp)).all(axis=1)\n",
    "\n",
    "        # Apply the mask to filter rows\n",
    "        filtered_df = df[mask]\n",
    "        \n",
    "        # Drop waveforms with peak beyond 40 ns\n",
    "        j_max = filtered_df.idxmin(axis=1)\n",
    "        \n",
    "        # Convert the index to numeric\n",
    "        j_max_numeric = pd.to_numeric(j_max, errors='coerce') * horizontalScaleFactor\n",
    "        \n",
    "        # Filter rows based on the peak time (j_max_numeric) being <= 40 ns\n",
    "        filtered_df = filtered_df[(j_max_numeric >= peakMinTimeThreshold)]\n",
    "        \n",
    "        # Drop waveforms with any value greater than the threshold beyond 50 ns\n",
    "        # Calculate the index position for 50 ns\n",
    "        time_50ns = int(timeThreshold / horizontalScaleFactor)\n",
    "                \n",
    "        # Apply filter: remove rows where any value after 50 ns exceeds threshold\n",
    "        filtered_df = filtered_df[filtered_df.iloc[:, time_50ns:].min(axis=1) >= ADCThreshold]\n",
    "        \n",
    "        # Save data\n",
    "        DATA[channel] = filtered_df\n",
    "        \n",
    "        dropped = df.shape[0] - filtered_df.shape[0]\n",
    "        print(f\"Dropped {dropped} waveforms from: {filterChannel}\")\n",
    "        \n",
    "    return DATA\n",
    "\n",
    "\n",
    "def averageWaveforms(Traw:pd.DataFrame, bin_size:int=100):\n",
    "    \"\"\"Average the waveforms from the raw dataframe into bins. \n",
    "\n",
    "    Args:\n",
    "        Traw (pd.DataFrame): Dataframe containing waveforms\n",
    "        bin_size (int, optional): Width of bin used for averaging. Defaults to 100. \n",
    "                                  None will average all waveforms to a single bin.\n",
    "\n",
    "    Returns:\n",
    "        Tav (pd.DataFrame): Average dataframe with bins of size bin_size.\n",
    "    \"\"\"\n",
    "    # Group rows into bins of size `bin_size` and calculate the mean for each bin\n",
    "    if bin_size != None:\n",
    "        Tav = Traw.groupby(Traw.index // bin_size).mean()\n",
    "    elif bin_size == 1:\n",
    "        Tav = Traw\n",
    "    else:\n",
    "        Tav = Traw.mean()\n",
    "        \n",
    "    # Reset the index for clarity\n",
    "    Tav.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return Tav\n",
    "\n",
    "\n",
    "def baselineWaveforms(Tin:pd.DataFrame, mode=\"poly\", regions=[(0, 50), (300, 512)]):\n",
    "    \"\"\"\n",
    "    Perform baseline correction for waveforms in a DataFrame by fitting and subtracting linear trends.\n",
    "\n",
    "    Args:\n",
    "        Tin (pd.DataFrame): Input DataFrame where each row represents a waveform, and columns represent time steps.\n",
    "        regions (list of tuples): List of regions (start, end) used to estimate the baseline. \n",
    "                                  Defaults to [(0, 50), (300, 512)].\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - out (pd.DataFrame): The baseline-corrected waveforms.\n",
    "            - pd.DataFrame: The fitted baseline values for each waveform.\n",
    "    \"\"\"\n",
    "    # Baseline Region: Combine specified regions\n",
    "    baselineRaw = pd.concat([Tin.iloc[:, regions[i][0]:regions[i][1]] for i in range(len(regions))], axis=1)\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        # Calculate the mean baseline for each row\n",
    "        meanBase = baselineRaw.mean(axis=1)\n",
    "        # Subtract mean baseline from the original data\n",
    "        result = Tin.sub(meanBase, axis=0)\n",
    "        return result, meanBase\n",
    "\n",
    "    elif mode == \"polyfit\":\n",
    "        # Concatenate the indices for the specified regions\n",
    "        X_fit = np.concatenate([np.arange(regions[i][0], regions[i][1]) for i in range(len(regions))])\n",
    "        # Get the raw NumPy array for baseline fitting\n",
    "        Y = baselineRaw.to_numpy()\n",
    "\n",
    "        # Initialize an array to store polynomial coefficients\n",
    "        coeffs = []\n",
    "        \n",
    "        # Compute polynomial fits\n",
    "        coeffs = Parallel(n_jobs=-1)(delayed(np.polyfit)(X_fit, y, deg=0) for y in Y)\n",
    "\n",
    "        # Convert the list of coefficients to a NumPy array\n",
    "        coeffs = np.array(coeffs)\n",
    "\n",
    "        # Evaluate the polynomial fits using np.polyval\n",
    "        X_val = np.arange(Tin.shape[1])\n",
    "        fitted_lines = np.array([np.polyval(p, X_val) for p in coeffs])\n",
    "\n",
    "        # Convert fitted lines back to a Dask DataFrame\n",
    "        fitted_lines_df = pd.DataFrame(fitted_lines, index=Tin.index, columns=Tin.columns)\n",
    "\n",
    "        # Subtract polynomial fit from the original data\n",
    "        result = Tin.sub(fitted_lines_df)\n",
    "\n",
    "        return result, pd.DataFrame(fitted_lines)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Use 'mean' or 'polyfit'.\")\n",
    "\n",
    "\n",
    "def displayWaveforms(T: dict, xscale=horizontalScaleFactor, title=\"\", vlines=None, save=True, showplot=False, savepath=\"Figures/\", rowwise=True, multi=False):\n",
    "    \"\"\"\n",
    "    Display multiple waveforms stored in a dictionary of DataFrames as subplots.\n",
    "\n",
    "    Args:\n",
    "        T (dict): Dictionary where keys are channel identifiers and values are DataFrames containing waveforms.\n",
    "        xscale (float): Scaling factor for the x-axis (time). Default is 1.0.\n",
    "        title (str): Title for the overall figure.\n",
    "        vlines (dict): Dictionary with channel names as keys and lists of vertical line positions as values.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the waveforms as a series of subplots.\n",
    "    \"\"\"\n",
    "    if not showplot:\n",
    "        plt.ioff()\n",
    "    \n",
    "    channels = list(T.keys())\n",
    "    nrows = int(len(channels) / 2) if len(channels) % 2 == 0 else int(len(channels) // 2 + 1)\n",
    "    ncols = 2\n",
    "    figsize = (16,10)\n",
    "    \n",
    "    if not rowwise:\n",
    "        ncols = nrows\n",
    "        nrows = 2\n",
    "        figsize = (10, 16)\n",
    "        \n",
    "    if not multi:\n",
    "        ncols = 1\n",
    "        nrows = 1\n",
    "    \n",
    "    # Create figure and axes for subplots\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows > 1 or ncols > 1:\n",
    "        axes = axes.ravel()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    cmap = plt.get_cmap('tab10')  # Use colormap for line colors\n",
    "    \n",
    "    # Determine the device for PyTorch\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(channels):\n",
    "            channel = channels[i]\n",
    "            df = T[channel].T\n",
    "\n",
    "            # Extract X and Y data\n",
    "            X = df.index.values * xscale  # Shape: (num_points,)\n",
    "            Y = df.values  # Shape: (num_points, num_waveforms)\n",
    "\n",
    "            print(f\"Plotting ID {channel} with {Y.shape[1]} waveform(s)...\")\n",
    "\n",
    "            # Prepare segments and colors\n",
    "            segments = []\n",
    "            colors = []\n",
    "\n",
    "            for idx, waveform in enumerate(Y.T):  # Iterate over each waveform (column of Y)\n",
    "                points = np.stack((X, waveform), axis=1)  # Shape: (num_points, 2)\n",
    "                segments.append(points)\n",
    "\n",
    "                # Assign color based on the waveform index\n",
    "                c  = random.random()\n",
    "                colors.append(cmap(c))\n",
    "\n",
    "            # Create a LineCollection from segments and colors\n",
    "            lc = mcoll.LineCollection(segments, linewidths=0.5, colors=colors, alpha=0.6, rasterized=True)\n",
    "            ax.add_collection(lc)\n",
    "            ax.autoscale()  # Automatically adjust limits to fit data\n",
    "\n",
    "            ax.set_title(f\"Run {channel}\")\n",
    "            ax.set_xlabel(\"Time (ns)\")\n",
    "            ax.set_ylabel(\"Amplitude\")\n",
    "\n",
    "            if vlines is not None and channel in vlines:\n",
    "                for v in vlines[channel]:\n",
    "                    ax.axvline(x=v * xscale, linestyle='--', color='black', alpha=0.6, rasterized=True, linewidth=0.7)\n",
    "        else:\n",
    "            ax.axis(\"off\")  # Turn off unused subplots if there are extra axes\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        fname = savepath+title.replace(\" \", \"_\")+\".png\"\n",
    "        print(f\"Saving figure to {fname}...\")\n",
    "        plt.savefig(fname, dpi=100)  # JPEG saves faster\n",
    "        print(\"Done\")\n",
    "        plt.close()\n",
    "    \n",
    "    if showplot:\n",
    "        print(\"Rendering figure\")\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "def analyzeWaveforms(Tin:pd.DataFrame, j_A:int=50, j_B:int=100):\n",
    "    \"\"\"\n",
    "    Analyze waveforms in a DataFrame and return results as a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        Tin (pd.DataFrame): Each row is a waveform; each column is a time step.\n",
    "        j_A (int): Number of time steps before the peak to include in integration.\n",
    "        j_B (int): Number of time steps after the peak to include in integration.\n",
    "    \n",
    "    Returns:\n",
    "        data (pd.DataFrame): DataFrame containing charge, amplitude, and timing.\n",
    "    \"\"\"\n",
    "    # Index of maximum value in pulse (for each row)\n",
    "    j_max = Tin.idxmax(axis=1)\n",
    "\n",
    "    # Timing (location of the pulse peak)\n",
    "    timing = j_max\n",
    "\n",
    "    # Nominal Baseline value (average of first 50 columns for each row)\n",
    "    baseline = Tin.iloc[:, 0:50].mean(axis=1)\n",
    "\n",
    "    # Amplitude of the pulse -- Define as height between maximum and the baseline\n",
    "    amplitude = Tin.max(axis=1) - baseline\n",
    "\n",
    "    # Create a mask for charge integration\n",
    "    A = (j_max - j_A).clip(lower=0)  # Ensure indices are not below 0\n",
    "    B = (j_max + j_B).clip(upper=Tin.shape[1])  # Ensure indices do not exceed max columns\n",
    "\n",
    "    # Vectorized integration\n",
    "    charge = [\n",
    "        (Tin.iloc[i, A[i]:B[i]] - baseline.iloc[i]).sum() for i in range(len(Tin))\n",
    "    ]\n",
    "\n",
    "    # Combine results into a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        \"charge\": charge,\n",
    "        \"amplitude\": amplitude,\n",
    "        \"timing\": timing\n",
    "    })\n",
    "\n",
    "    return data, (A.mean(), B.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Calibration Data\n",
    "\n",
    "- Calibration data is saved in individual files. The data is extracted and filtered using the `filterSignal()` function.\n",
    "- Calibration data is then averaged using the `averageWaveform()` function into bins of `1000` samples.\n",
    "- Then, a baseline reduction is performed on the waveforms using the `baseline()` function. This subtracts out a 1 degree polynomial fit from the baseline.\n",
    "- Finally, the signal is flipped so it the information from the waveform can be extracted.\n",
    "- Visualisations of the the data is produced at each step (when applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = IntProgress(min=0, max=(len(paths))*5)\n",
    "display(progress)\n",
    "\n",
    "ID = Run_IDs\n",
    "\n",
    "# Load and filter signals\n",
    "Traw_cal = {i: None for i in ID}\n",
    "T = {i: None for i in ID}  # For the plot\n",
    "\n",
    "highpass = {\"Ch0\": 0}\n",
    "for path in paths:\n",
    "    Traw_cal[PathRunMap[path]] = filterSignal(path, [\"Ch0\"], highpass=0, lowpass=5000, timeThreshold=0, ADCThreshold=0, peakMinTimeThreshold=0)[\"Ch0\"]  # Filtered\n",
    "    progress.value += 5\n",
    "    \n",
    "# Average waveforms\n",
    "Tav_cal = {ch: averageWaveforms(Traw_cal[ch], bin_size = 1) for ch in ID}\n",
    "\n",
    "progress.value += 5\n",
    "\n",
    "# Baseline reduction of waveforms\n",
    "Tbase_cal = {ch: None for ch in ID}\n",
    "fitted_baseline_cal = {ch: None for ch in ID}\n",
    "for ch in ID:\n",
    "    Tbase_cal[ch], fitted_baseline_cal[ch] = baselineWaveforms(Tav_cal[ch], mode=\"polyfit\")#, regions=[(0, 10)])#, (300, 512)])#, regions=[(0, 50)])\n",
    "    progress.value += 5\n",
    "\n",
    "# Update progress bar\n",
    "progress.value += len(ID) * 5\n",
    "\n",
    "# Flip the waveforms for the data extraction from the waveforms\n",
    "Tflip_cal = {ch: -1*Tbase_cal[ch] for ch in ID}\n",
    "\n",
    "# Save data as figures\n",
    "save=False\n",
    "if save:\n",
    "    XY = {\"Ch0\": Traw_cal[\"Ch0\"].iloc[0:1000]}\n",
    "    XY2 = {\"Ch0\": fitted_baseline_cal[\"Ch0\"].iloc[0:1000]}\n",
    "    # Display Filtered waveforms\n",
    "    # displayWaveforms(XY, title=Run + \" Raw Sensitivity\", save=True)\n",
    "\n",
    "    # Display averaged waveforms\n",
    "    displayWaveforms(Tav_cal, title=\"Processed Averaged Waveforms\", save=True)\n",
    "\n",
    "    # Display the baseline fit that was subtracted out\n",
    "    # displayWaveforms(XY2, title= Run+\" Baselined Sensitivity\", save=True)\n",
    "\n",
    "    # Display baseline reduced waveforms\n",
    "    displayWaveforms({\"Ch0\": fitted_baseline_cal[\"Ch0\"].iloc[0:10]}, title=\"Baselined Sensitivity Fit\", save=False)\n",
    "    \n",
    "\n",
    "progress.value += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(Tflip_cal, open(\"Tflip_cal.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information from the Calibration Waveforms  \n",
    "- Pull out the amplitude, charge, and timing from the data. Uses the `analyzeWaveforms()` function.\n",
    "- Find the average charge, amplitudem and timing for each channel (i.e, each detector in the calibration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parallelized function\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(analyzeWaveforms)(Tflip_cal[ch], j_A=60, j_B=60) for ch in ID\n",
    ")\n",
    "\n",
    "# Collect the results\n",
    "data_cal = {ch: result[0] for ch, result in zip(ID, results)}\n",
    "J = {ch: result[1] for ch, result in zip(ID, results)}  \n",
    "\n",
    "# # Calculate the mean value for each channel\n",
    "# meanData_cal = {ch: data_cal[ch].mean() for ch in CH}\n",
    "# medData_cal = {ch: data_cal[ch].median() for ch in CH}\n",
    "# # displayWaveforms(Tflip_cal,  vlines=J, title=\"Processed Sr-90 Waveforms No Average\")\n",
    "\n",
    "# # Convert to a dataframe\n",
    "# meanData_cal = pd.DataFrame(meanData_cal).T\n",
    "# medData_cal = pd.DataFrame(medData_cal).T\n",
    "# pd.concat([meanData_cal, medData_cal], axis=1)\n",
    "\n",
    "# # Charge to Energy scalar:\n",
    "# scale_factor = [2.2/data_cal[ch][\"charge\"].max() for ch in CH]\n",
    "\n",
    "scale_factor = 1\n",
    "\n",
    "# Display baseline reduced waveforms\n",
    "XY = {ID[0]: Tflip_cal[ID[0]].iloc[0:1000]}\n",
    "\n",
    "displayWaveforms(XY, title=ID[0]+\" Flipped Sensitivity\", save=True, vlines=J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduced_cal = {i: Tflip_cal[i].iloc[0:1000] for i in ID}\n",
    "\n",
    "displayWaveforms(Reduced_cal, title=\"Flipped Sensitivity\", save=True, vlines=J, multi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Energy Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import simpson\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# Assuming 'hist' is the histogram data and 'bin_centers' are the corresponding x values\n",
    "def integrate_spectrum(bin_centers, hist, fit_function=None, popt=None):\n",
    "    \"\"\"\n",
    "    Integrate the spectrum either directly from the histogram or using the fit function.\n",
    "    \n",
    "    Parameters:\n",
    "    - bin_centers: The bin center values (x-axis)\n",
    "    - hist: The histogram values (y-axis)\n",
    "    - fit_function: The fit function (e.g., Gaussian or double Gaussian) [Optional]\n",
    "    - popt: The parameters of the fit function [Optional]\n",
    "    \n",
    "    Returns:\n",
    "    - integral: The area under the curve\n",
    "    \"\"\"\n",
    "    if fit_function and popt is not None:\n",
    "        # Use the fit function to calculate the curve\n",
    "        y_fit = fit_function(bin_centers, *popt)\n",
    "        integral = simpson(y_fit, x=bin_centers)\n",
    "    else:\n",
    "        # Integrate the raw histogram\n",
    "        integral = simpson(hist, x=bin_centers)\n",
    "    \n",
    "    return integral\n",
    "\n",
    "\n",
    "def gaussian(x, amp, mu, sigma):\n",
    "    return amp * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "\n",
    "# Set up the subplots\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 5), sharey=True)\n",
    "if nrows > 1 and ncols > 1:\n",
    "    axes = axes.ravel()\n",
    "else:\n",
    "    axes = [axes]\n",
    "\n",
    "MPV_dict = {ch: None for ch in ID}  # Dictionary to store MPVs\n",
    "bins = 2000  # Number of bins\n",
    "i = 0\n",
    "\n",
    "for ax in axes:\n",
    "    # Create a histogram\n",
    "    hist, bin_edges = np.histogram(data_cal[ID[i]][\"charge\"]*scale_factor, bins=bins, density=True)\n",
    "        \n",
    "    # Calculate bin centers\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Crop out negative values\n",
    "    hist = hist[(bin_centers > 0) & (bin_centers < 800)]\n",
    "    bin_centers = bin_centers[(bin_centers > 0) & (bin_centers < 800)]\n",
    "        \n",
    "    # Plot the histogram\n",
    "    ax.plot(bin_centers, hist, label=\"Sensitivity\")\n",
    "    ax.set_title(f\"Run {ID[i]}\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    \n",
    "    if i > 5:\n",
    "        ax.set_xlabel(\"Charge (arb)\")\n",
    "    \n",
    "    try:\n",
    "        # Initial guesses for the two peaks (adjust based on your data):\n",
    "        p0 = [\n",
    "            np.max(hist[:]),                      # Amplitude of peak 1\n",
    "            bin_centers[:][np.argmax(hist[:])],     # Mean of peak 1\n",
    "            200,                             # Standard deviation of peak 1\n",
    "        ]\n",
    "        \n",
    "        # Fit the spectrum (double Gaussian example)\n",
    "        popt, pcov = curve_fit(gaussian, bin_centers, hist, p0=p0)  # Provide initial guesses\n",
    "        \n",
    "        # Integrate the fitted curve\n",
    "        integral_fit = integrate_spectrum(bin_centers, hist, fit_function=gaussian, popt=popt)\n",
    "        \n",
    "        # Integrate the raw histogram\n",
    "        integral_raw = integrate_spectrum(bin_centers, hist)\n",
    "        \n",
    "        print(f\"Channel {ID[i]}: Total Integral (Fit): {integral_fit}, Raw Histogram: {integral_raw}\")\n",
    "        \n",
    "        # Extract the means (mu1, mu2) as the most probable values for each peak\n",
    "        mpv1 = popt[1]\n",
    "        mu1_error = np.sqrt(pcov[1, 1])\n",
    "        \n",
    "        print(popt)\n",
    "        \n",
    "        MPV_dict[ID[i]] = {\"MPV1\": mpv1, \"MPV1_error\": mu1_error, \"Integral\": integral_fit}\n",
    "        \n",
    "        print(f\"Channel {ID[i]}: MPV1 = {mpv1} ± {mu1_error}\")\n",
    "        \n",
    "        # Plot the fitted curve and histogram\n",
    "        ax.plot(bin_centers, gaussian(bin_centers, *popt), 'r--', label=\"Fit\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unable to fit Channel {ID[i]}: {e}\")\n",
    "    \n",
    "    \n",
    "    # Print average energy\n",
    "    # Calculate bin centers\n",
    "    # bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    # Calculate the average x-value\n",
    "    average_x = np.sum(bin_centers * hist) / np.sum(hist)\n",
    "    \n",
    "    # ax.vlines(mpv1, hist.min(), hist.max(), color=\"black\", linestyles=\"--\", label=\"<E>={:.2f} MeV\".format(average_x))\n",
    "    # ax.set_yscale('log')\n",
    "    # ax.legend()\n",
    "    print(average_x)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Set the overall title for the figure\n",
    "# fig.suptitle(\"Muon Spectrum\", fontsize=16)\n",
    "\n",
    "# Tight layout for spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"Figures/Charge.png\", dpi=300)\n",
    "# # Scale MPVs based on calibration\n",
    "# mpv_scaled = [(MPV_dict[CH[i]][0] / CalScale[i], MPV_dict[CH[i]][1] / CalScale[i]) for i in range(4)]\n",
    "# print(mpv_scaled)\n",
    "\n",
    "print(MPV_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPVs = [MPV_dict[ID[i]][\"Integral\"] for i in range(len(ID))]\n",
    "Locs = [RunLocMap[ID[i]] for i in range(len(ID))]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(Locs, MPVs)\n",
    "plt.savefig(\"MPVs.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
